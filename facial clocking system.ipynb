{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aee2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65fdc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2441b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4daa153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aparna Anand\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aparna Anand\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88eb4ac7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, None, None, 3)     1731      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114307 (446.51 KB)\n",
      "Trainable params: 114307 (446.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1015 - accuracy: 0.1649\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0939 - accuracy: 0.3749\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0857 - accuracy: 0.2889\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0811 - accuracy: 0.2210\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0834 - accuracy: 0.4354\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0800 - accuracy: 0.5944\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0773 - accuracy: 0.5952\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0778 - accuracy: 0.5948\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0752 - accuracy: 0.5933\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0717 - accuracy: 0.5923\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0688 - accuracy: 0.6070\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0654 - accuracy: 0.5795\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0620 - accuracy: 0.5514\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0574 - accuracy: 0.5342\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0528 - accuracy: 0.5035\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0480 - accuracy: 0.4859\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0453 - accuracy: 0.4896\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0377 - accuracy: 0.4621\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0302 - accuracy: 0.4761\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0247 - accuracy: 0.4786\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0224 - accuracy: 0.4786\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0271 - accuracy: 0.5023\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0164 - accuracy: 0.4985\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0259 - accuracy: 0.5021\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0285 - accuracy: 0.5368\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0295 - accuracy: 0.5461\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0152 - accuracy: 0.5558\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0330 - accuracy: 0.5694\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - accuracy: 0.5908\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0253 - accuracy: 0.6222\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0198 - accuracy: 0.6420\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - accuracy: 0.6597\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0214 - accuracy: 0.6608\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0093 - accuracy: 0.6942\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - accuracy: 0.7191\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0147 - accuracy: 0.6838\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0096 - accuracy: 0.7013\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0078 - accuracy: 0.7419\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0123 - accuracy: 0.7623\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0078 - accuracy: 0.7388\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0071 - accuracy: 0.7282\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0093 - accuracy: 0.7248\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0080 - accuracy: 0.7324\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0061 - accuracy: 0.7507\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0077 - accuracy: 0.7552\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0075 - accuracy: 0.7611\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0058 - accuracy: 0.7745\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0066 - accuracy: 0.7559\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0067 - accuracy: 0.7563\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0057 - accuracy: 0.7811\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0056 - accuracy: 0.7798\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0060 - accuracy: 0.7765\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0056 - accuracy: 0.7755\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0052 - accuracy: 0.7454\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0054 - accuracy: 0.7342\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0053 - accuracy: 0.7268\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0049 - accuracy: 0.7275\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0049 - accuracy: 0.7364\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0049 - accuracy: 0.7466\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0047 - accuracy: 0.7708\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0047 - accuracy: 0.7695\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0046 - accuracy: 0.7753\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0045 - accuracy: 0.7821\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0046 - accuracy: 0.7893\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0044 - accuracy: 0.7889\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0044 - accuracy: 0.7844\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0044 - accuracy: 0.7797\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0043 - accuracy: 0.7766\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0043 - accuracy: 0.7748\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0043 - accuracy: 0.7745\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0042 - accuracy: 0.7758\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0042 - accuracy: 0.7772\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0041 - accuracy: 0.7780\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0041 - accuracy: 0.7719\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0040 - accuracy: 0.7692\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0040 - accuracy: 0.7671\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0040 - accuracy: 0.7666\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0039 - accuracy: 0.7672\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0039 - accuracy: 0.7692\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0038 - accuracy: 0.7729\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0038 - accuracy: 0.7762\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0037 - accuracy: 0.7702\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0037 - accuracy: 0.7664\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0037 - accuracy: 0.7650\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0036 - accuracy: 0.7669\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0036 - accuracy: 0.7714\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0036 - accuracy: 0.7745\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0035 - accuracy: 0.7723\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0035 - accuracy: 0.7687\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0034 - accuracy: 0.7690\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0034 - accuracy: 0.7729\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0033 - accuracy: 0.7760\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0033 - accuracy: 0.7764\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0033 - accuracy: 0.7734\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0033 - accuracy: 0.7685\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0032 - accuracy: 0.7762\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0031 - accuracy: 0.7790\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0031 - accuracy: 0.7779\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0031 - accuracy: 0.7765\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0030 - accuracy: 0.7734\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Enhancement completed and images saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')  # Output layer with 3 channels for RGB\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape=(None, None, 3))  # Define input shape as (height, width, channels)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# # Load and preprocess images from the folder\n",
    "# def load_and_preprocess_images(folder_path):\n",
    "#     images = []\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust for other image formats if needed\n",
    "#             image_path = os.path.join(folder_path, filename)\n",
    "#             image = plt.imread(image_path)\n",
    "#             image = image / 255.0  # Normalize pixel values to range [0, 1]\n",
    "#             images.append(image)\n",
    "#     return np.array(images)\n",
    "\n",
    "def load_and_preprocess_images(folder_path, target_size=(256, 256)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = plt.imread(image_path)\n",
    "            image = tf.image.resize(image, target_size)  # Resize image\n",
    "            image = image / 255.0  # Normalize pixel values to range [0, 1]\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "# Enhance images using the model\n",
    "def enhance_images(model, images):\n",
    "    enhanced_images = []\n",
    "    for image in images:\n",
    "        input_image = np.expand_dims(image, axis=0)\n",
    "        enhanced_image = model.predict(input_image)[0]\n",
    "        enhanced_images.append(enhanced_image)\n",
    "    return np.array(enhanced_images)\n",
    "\n",
    "# Define folder path containing images\n",
    "folder_path = \"D:/projects/faceclock/photos\"\n",
    "\n",
    "# Load images from the folder\n",
    "images = load_and_preprocess_images(folder_path)\n",
    "\n",
    "# Check if any images are found\n",
    "if len(images) == 0:\n",
    "    print(\"No images found in the folder.\")\n",
    "else:\n",
    "    # Create the model\n",
    "    model = create_model(images[0].shape)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics='accuracy')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(images, images, epochs=100, verbose=1)\n",
    "\n",
    "    # Enhance images\n",
    "    enhanced_images = enhance_images(model, images)\n",
    "\n",
    "    # Save enhanced images to a folder (optional)\n",
    "    preprocessed_folder = \"D:/projects/faceclock/preprocessed_photos\"\n",
    "    os.makedirs(preprocessed_folder, exist_ok=True)\n",
    "    for idx, enhanced_image in enumerate(enhanced_images):\n",
    "        plt.imsave(os.path.join(preprocessed_folder, f\"enhanced_image_{idx}.jpg\"), enhanced_image)\n",
    "\n",
    "    print(\"Enhancement completed and images saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49854b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the path to the folder containing employee photos\n",
    "# photos_folder = \"D:/projects/faceclock/photos\"\n",
    "\n",
    "# # Define the path where preprocessed images will be saved\n",
    "# preprocessed_folder = \"D:/projects/faceclock/preprocessed_photos\"\n",
    "\n",
    "# # Function to copy original images to the preprocessed folder\n",
    "# def copy_images():\n",
    "#     # Create the preprocessed folder if it doesn't exist\n",
    "#     if not os.path.exists(preprocessed_folder):\n",
    "#         os.makedirs(preprocessed_folder)\n",
    "\n",
    "#     for filename in os.listdir(photos_folder):\n",
    "#         # Read the image path\n",
    "#         img_path = os.path.join(photos_folder, filename)\n",
    "\n",
    "#         # Save the original image to the preprocessed folder\n",
    "#         destination_path = os.path.join(preprocessed_folder, filename)\n",
    "#         shutil.copyfile(img_path, destination_path)\n",
    "\n",
    "# # Call the function to copy the original images\n",
    "# copy_images()\n",
    "\n",
    "# print(\"Images copied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d67c1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face recognition model trained successfully and saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to the folder containing preprocessed images\n",
    "preprocessed_folder = \"D:/projects/faceclock/preprocessed_photos\"\n",
    "\n",
    "# Function to extract labels and preprocessed images\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_id = 0\n",
    "    label_ids = {}  # Dictionary to map labels to integer IDs\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "        if img is not None:\n",
    "            labels.append(os.path.splitext(filename)[0])  # Extract label from filename\n",
    "            # Assign a unique ID to each unique label\n",
    "            if labels[-1] not in label_ids:\n",
    "                label_ids[labels[-1]] = label_id\n",
    "                label_id += 1\n",
    "            images.append(img)\n",
    "    return images, labels, label_ids\n",
    "\n",
    "# Load preprocessed images, labels, and label IDs\n",
    "images, labels, label_ids = load_images_from_folder(preprocessed_folder)\n",
    "\n",
    "# Convert labels to numpy array and specify data type as int32\n",
    "labels_ids = np.array([label_ids[label] for label in labels])\n",
    "labels_ids = labels_ids.astype(np.int32)\n",
    "\n",
    "# Initialize LBPH face recognizer\n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Train the face recognizer\n",
    "face_recognizer.train(images, labels_ids)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = \"D:/projects/faceclock/face_recognition_model.yml\"\n",
    "face_recognizer.save(model_path)\n",
    "\n",
    "print(\"Face recognition model trained successfully and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11cbb50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee data inserted successfully into the database!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to the MySQL database\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"aparna\",\n",
    "    database=\"employeeAttendance\"\n",
    ")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Path to the folder containing preprocessed images\n",
    "preprocessed_folder = \"D:/projects/faceclock/preprocessed_photos\"\n",
    "\n",
    "# Iterate through the files in the preprocessed photos folder\n",
    "for filename in os.listdir(preprocessed_folder):\n",
    "    img_path = os.path.join(preprocessed_folder, filename)\n",
    "    if os.path.isfile(img_path):\n",
    "        # Extract employee name from the filename (assuming the filename is the employee name)\n",
    "        empname, _ = os.path.splitext(filename)\n",
    "        \n",
    "        # Construct the SQL query to insert data into the Employee table\n",
    "        sql_query = \"INSERT INTO Employee (empname, imgpath) VALUES (%s, %s)\"\n",
    "        \n",
    "        # Execute the SQL query\n",
    "        cursor.execute(sql_query, (empname, img_path))\n",
    "        \n",
    "        # Commit the transaction\n",
    "        db_connection.commit()\n",
    "\n",
    "# Close the cursor and database connection\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "print(\"Employee data inserted successfully into the database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b94c4c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Aparna (ID: 2) clocked in at 12:56:38 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked out at 12:56:41 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked in at 12:56:41 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked out at 12:56:44 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked in at 12:56:44 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked out at 12:56:46 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked in at 12:56:49 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked out at 12:56:50 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked in at 12:56:52 on 2024-03-07\n",
      "Employee Aparna (ID: 2) clocked out at 12:56:54 on 2024-03-07\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import datetime\n",
    "\n",
    "# Connect to the MySQL database\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"aparna\",\n",
    "    database=\"employeeattendance\"\n",
    ")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Load the trained face recognition model\n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "face_recognizer.read(\"D:/projects/faceclock/face_recognition_model.yml\")\n",
    "\n",
    "# Load the pre-trained face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Dictionary to track the clock-in status of each employee for the current day\n",
    "clock_status = {}\n",
    "\n",
    "# Flag to track whether an employee's attendance has been marked in the current frame\n",
    "attendance_marked = {}\n",
    "\n",
    "# Counter to track the number of times an employee is detected\n",
    "employee_counter = {}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    label, confidence = face_recognizer.predict(gray)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    if label != -1:  # If a face is recognized\n",
    "        sql_query = \"SELECT empid, empname FROM employee WHERE empid = %s\"\n",
    "        cursor.execute(sql_query, (label,))\n",
    "        employee_data = cursor.fetchone()\n",
    "\n",
    "        if employee_data:\n",
    "            empid, empname = employee_data\n",
    "            current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "            # Initialize clock_status, employee_counter, and attendance_marked for new employees\n",
    "            if empid not in clock_status:\n",
    "                clock_status[empid] = {\"status\": \"out\"}\n",
    "                employee_counter[empid] = 0\n",
    "                attendance_marked[empid] = False\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                # If attendance is not marked for the current frame, mark attendance based on counter\n",
    "                if not attendance_marked[empid]:\n",
    "                    employee_counter[empid] += 1\n",
    "\n",
    "                    # If employee is detected for the first time or odd number of times, mark clock in\n",
    "                    if employee_counter[empid] % 2 == 1:\n",
    "                        if clock_status[empid][\"status\"] == \"out\":\n",
    "                            clock_status[empid][\"status\"] = \"in\"\n",
    "                            sql_insert = \"INSERT INTO records (empid, empname, date, clockin) VALUES (%s, %s, %s, %s)\"\n",
    "                            cursor.execute(sql_insert, (empid, empname, current_date, current_time))\n",
    "                            db_connection.commit()\n",
    "                            print(f\"Employee {empname} (ID: {empid}) clocked in at {current_time} on {current_date}\")\n",
    "                        attendance_marked[empid] = True\n",
    "\n",
    "                    # If employee is detected for the second time or even number of times, mark clock out\n",
    "                    else:\n",
    "                        if clock_status[empid][\"status\"] == \"in\":\n",
    "                            clock_status[empid][\"status\"] = \"out\"\n",
    "                            sql_update = \"UPDATE records SET clockout = %s WHERE empid = %s AND date = %s AND clockout IS NULL\"\n",
    "                            cursor.execute(sql_update, (current_time, empid, current_date))\n",
    "                            db_connection.commit()\n",
    "                            print(f\"Employee {empname} (ID: {empid}) clocked out at {current_time} on {current_date}\")\n",
    "                        attendance_marked[empid] = True\n",
    "\n",
    "            else:\n",
    "                # Reset attendance marking flag when employee leaves the frame\n",
    "                attendance_marked[empid] = False\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        color = (0, 255, 0)  # Green color for recognized faces\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "    cv2.imshow('Employee Clocking System', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Close the cursor and database connection\n",
    "cursor.close()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67365e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
